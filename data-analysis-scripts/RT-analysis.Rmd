---
title: "Response Time Analysis"
output: html_document
date: "2025-12-07"
---

## Packages

```{r}
# import packages
library(lme4)     # mixed models
library(dplyr)    # dataframe manipulation
library(readr)    # reading in the data
library(emmeans)  # likelihood ratio testing
library(purrr)    # for map()
library(lattice)  # for xyplot()
library(TAF)      # setting up output file directory structure during model fitting
library(moments)  # skewness for selecting between additive and multiplicative models of response times
library(performance)

library(ggplot2)


#If you get an error, you likely need to install the packages using the "install.package('insert-package-name-here')" command
```


## Notebook configurable settings

```{r}
# name the directories to read data from and write output to
input_version = 2
output_version = 'programming'

```

make an overall output directory (model summaries, stats, etc. will all go here)
```{r}

outputdir = sprintf("%s/../candidate-models/regression-models-%s",getwd(), output_version)
#mkdir(outputdir) #only uncomment this if you don't already have an output directory
```




## Loading Data

```{r}

input_file = sprintf("%s/../preprocessed-data/full-data-version-%d/programming-full-data.csv", getwd(), input_version)
data <- read.csv(input_file)

#showing head of data
head(data)

```


### Data Management


Computing the IPAQ score and adding it as a column to the data
```{r Nans}

# Helper to force numeric and turn NAs/blanks into 0
nz <- function(x) {
  x <- suppressWarnings(as.numeric(x))
  x[is.na(x)] <- 0
  x
}

# Coerce all inputs
data$days_vigorous    <- nz(data$days_vigorous)
data$minutes_vigorous <- nz(data$minutes_vigorous)
data$days_moderate    <- nz(data$days_moderate)
data$minutes_moderate <- nz(data$minutes_moderate)
data$days_walking     <- nz(data$days_walking)
data$minutes_walking  <- nz(data$minutes_walking)   # important: was <chr>
data$hours_sitting    <- nz(data$hours_sitting)

# MET constants from IPAQ scoring guidelines
MET_VIG  <- 8.0
MET_MOD  <- 4.0
MET_WALK <- 3.3

# Component MET-min/week scores
data$vigorous_met <- ifelse(
  data$days_vigorous > 0 & data$minutes_vigorous > 0,
  data$days_vigorous * data$minutes_vigorous * MET_VIG,
  0
)

data$moderate_met <- ifelse(
  data$days_moderate > 0 & data$minutes_moderate > 0,
  data$days_moderate * data$minutes_moderate * MET_MOD,
  0
)

data$walking_met <- ifelse(
  data$days_walking > 0 & data$minutes_walking > 0,
  data$days_walking * data$minutes_walking * MET_WALK,
  0
)

# Total IPAQ MET-min/week
data$ipa_total_met <- data$vigorous_met + data$moderate_met + data$walking_met

# Number of days doing ANY activity (required for categorization)
data$total_days_any <- data$days_vigorous + data$days_moderate + data$days_walking

# IPAQ categorical classification HIGH / MODERATE / LOW
data$ipaq_category <- "Low"   # default

# High:
data$ipaq_category[
  (data$days_vigorous >= 3 & data$ipa_total_met >= 1500) |
  (data$total_days_any >= 7 & data$ipa_total_met >= 3000)
] <- "High"

# Moderate (only if not High):
data$ipaq_category[
  data$ipaq_category != "High" &
  (
    (data$days_vigorous >= 3 & data$minutes_vigorous >= 20) |
    (data$days_moderate >= 5 & data$minutes_moderate >= 30) |
    (data$days_walking  >= 5 & data$minutes_walking  >= 30) |
    (data$total_days_any >= 5 & data$ipa_total_met >= 600)
  )
] <- "Moderate"

# vigorous_met, moderate_met, walking_met, ipa_total_met, ipaq_category

# Optional quick checks:
table(data$ipaq_category)
summary(data$ipa_total_met)



```

#### Filtering data to correct trials

Lets only accept the trials that led to 3.5+ scores on all categories
(as an extra layer, lets remove people who said they didn't read the AI)
```{r}

df_correct <- data |> 
  dplyr::filter( accuracy_both >= 3.5,
                completeness_both >= 3.5,
                conciseness_both >= 3.5,
                readability_both >= 3.5
                )

#df_correct <- subset(df_correct, zoom_self != "self")
df_correct <- subset(df_correct, read_ai != "No")

head(df_correct)

```

How many unique participants are left
```{r}

id <- unique(df_correct$part_id)
id #all unique IDs
length(id) #number of participants
```


### Distribution of Response Times


Overall average response time
```{r}
mean_value <- mean((data$duration))
print(mean_value)
```


Centralizing dureciton/reaction times where not normally distrubuted so that were transformed
```{r}
hist((df_correct$duration))
hist(sqrt(df_correct$duration))

#table(df_correct$exercise, df_correct$ai)

```

## Select independent variables 


## Specify candidate predictor structures for mixed model formulas

```{r}

# Always include (core)
core_mains <- c("exercise", "ai", "seq_num")

# covariates
optional_mains <- c("project_name", "ipaq_category", "occupation")

# Optional interactions
candidate_interactions <- c(
  "exercise * ai",
  "exercise * ipaq_category",
  # "ai * ipaq_category"       
  "ai * seq_num",
  "exercise * seq_num" # learning-curve moderation
  #"ai * has_bug",
  #"exercise * has_bug"
)
```


Geneate a plausible fixed effects structures given that we want to keep 3 core fixed effects but what to extend it
```{r}
make_fixed_forms <- function(core_mains, optional_mains, candidate_interactions) {
  fixed_forms <- character(0)
  
  ## 1) Parse interactions into list(name, vars)
  # e.g. "exercise * ai" -> name = "exercise:ai", vars = c("exercise","ai")
  interaction_defs <- lapply(candidate_interactions, function(trm) {
    vars <- trimws(strsplit(trm, "\\*")[[1]])
    list(
      name = paste(vars, collapse = ":"),  # use ":" in the formula
      vars = vars
    )
  })
  
  ## 2) All subsets of optional mains (including empty set)
  opt_subsets <- list(character(0))
  if (length(optional_mains) > 0) {
    for (k in 1:length(optional_mains)) {
      opt_subsets <- c(
        opt_subsets,
        combn(optional_mains, k, simplify = FALSE)
      )
    }
  }
  
  ## 3) All subsets of interactions (including empty)
  int_subsets <- list(integer(0))  # each element is a vector of indices into interaction_defs
  n_int <- length(interaction_defs)
  if (n_int > 0) {
    for (k in 1:n_int) {
      int_subsets <- c(
        int_subsets,
        combn(seq_len(n_int), k, simplify = FALSE)
      )
    }
  }
  
  ## 4) Build RHS strings
  for (opt_set in opt_subsets) {
    for (int_idx_set in int_subsets) {
      
      # which interactions are in this candidate?
      if (length(int_idx_set) == 0) {
        int_terms <- character(0)
        int_vars  <- character(0)
      } else {
        int_defs  <- interaction_defs[int_idx_set]
        int_terms <- vapply(int_defs, `[[`, character(1), "name")
        int_vars  <- unique(unlist(lapply(int_defs, `[[`, "vars")))
      }
      
      # all main-effect variables present in this model
      mains_here <- unique(c(core_mains, opt_set))
      
      # hierarchy rule: only keep models where all interaction vars are in mains
      if (!all(int_vars %in% mains_here)) {
        next
      }
      
      # assemble full RHS term list
      terms <- c(core_mains, opt_set, int_terms)
      rhs   <- paste(terms, collapse = " + ")
      fixed_forms <- c(fixed_forms, rhs)
    }
  }
  
  unique(fixed_forms)
}

fixed_forms <- make_fixed_forms(core_mains, optional_mains, candidate_interactions)
fixed_effects <- fixed_forms
#fixed_effects
```




## Specify candidate random effect structures for mixed model formulas

```{r}

random_structures <- c(
  R01 = "(1 | part_id)", 
  R02 = "(1 | stim_src_id)", 
  R0  = "(1 | part_id) + (1 | stim_src_id)",                 # intercepts only
  R1  = "(1 + ai | part_id) + (1 | stim_src_id)",            # AI slope
  R2  = "(1 + exercise | part_id) + (1 | stim_src_id)",      # exercise slope
  R3  = "(1 + ai + exercise | part_id) + (1 | stim_src_id)" # both slopes (correlated)
  #R3u = "(1 + ai + exercise || part_id) + (1 | stim_src_id)" # both slopes, no corr
)
```


Checking what random effect model is best given our core fixed effects and DV

```{r}


## 2. Main fixed-effects part of the model
fixed_main <- "sqrt(duration) ~ exercise + ai + seq_num"


## 4. Helper that fits all random structures and catches errors
fit_random_structures <- function(df, fixed_main, random_structures) {
  fits <- lapply(random_structures, function(re) {
    f_txt <- paste(fixed_main, "+", re)
    f <- as.formula(f_txt)
    message("\nFitting: ", f_txt)

    fit <- tryCatch(
      {
        lmer(f, data=df_correct, REML=FALSE)
      },
      error = function(e) {
        message("  ERROR: ", e$message)
        return(NULL)
      },
      warning = function(w) {
        message("  WARNING: ", w$message)
        invokeRestart("muffleWarning")  # show once, then silence
      }
    )

    fit
  })

  names(fits) <- random_structures
  fits
}

## 5. Run the fits

m_rand <- fit_random_structures(df, fixed_main, random_structures)

## 6. Drop models that failed (NULL)
m_rand_ok <- Filter(Negate(is.null), m_rand)

## 7. Compare AIC and BIC
aic_tab <- sapply(m_rand_ok, AIC)
bic_tab <- sapply(m_rand_ok, BIC)

#cat("\nAIC by random structure:\n")
#print(aic_tab)

#cat("\nBIC by random structure:\n")
#print(bic_tab)

## 8. Pick the best random structure by AIC (or BIC)
best_by_aic <- names(aic_tab)[which.min(aic_tab)]
best_by_bic <- names(aic_tab)[which.min(bic_tab)]
cat("\nBest random structure by AIC:", best_by_aic, "\n")
cat("\nBest random structure by BIC:", best_by_bic, "\n")

#best_model <- m_rand_ok[[best_by_aic]]

## 9. Inspect the chosen model

#summary(best_model)


```



```{r}

random_exprs = "(1 | part_id) + (1 | stim_src_id)"    #for reaction_time
  
```



Now, we combine the response variable, fixed effects, and random effects together to generate our candidate model structures.

```{r}
loglinear_formulae = c()
for (fe in fixed_effects) {
  for (re in random_exprs) {
    f = ""
    if (fe == "") {
      f = sprintf("sqrt(duration) ~ %s", re)
    } else {
      f = sprintf("sqrt(duration) ~ %s + %s", fe, re)
    }
    loglinear_formulae = append(loglinear_formulae, f)
  }
}

print(length(loglinear_formulae))
```


### Model fitting (\~2 minutes)

Here, we fit all the models. Of note, we use MLE rather than REML for compatibility with the AIC model selection strategy.

We also store BIC (an alternative model selection criterion) in addition to AIC here. The two have different theoretical properties, both of which may be useful. Both describe the log likelihood of a fitted model, with a penalty for model complexity. The penalty in BIC is higher than that for AIC, so optimal-BIC models tend to be smaller (with fewer parameters) than optimal-AIC models. We have done our analysis relying on AIC, but we provide BIC-based model selection here in case the reader is interested.

```{r eval=FALSE}
if (TRUE) {
  # make an output directory for the output of all the candidate linear models
  logtime_outputdir = 
    sprintf("%s/sqrt_time_model_correct_summaries_only_frozen_re", outputdir)
  mkdir(logtime_outputdir)
  
  # make an output file for the AIC and BIC of each model
  metricsfile = sprintf("%s/metrics.csv", logtime_outputdir)
  file.create(metricsfile)
  
  sink(file = metricsfile,
       append = FALSE,
       type = "output",
       split = FALSE
       )
  
  cat("formula,AIC,BIC\n")
  
  sink()
  
  for (f in loglinear_formulae) {
    # make an output directory for the output of this specific candidate model
    f_outputdir = sprintf("%s/%s", logtime_outputdir, gsub(" ", "", f, fixed = TRUE))
    mkdir(f_outputdir)
    
    # make the individual output file
    summaryfile = sprintf("%s/summary.txt", f_outputdir)
    
    file.create(summaryfile)
    
    # print a delimiter for console output
    cat(sprintf("\n---------------------------------------------\n%s\n", f))
    
    # fit the model
    m = lmer(f, data=df_correct, REML=FALSE)
    
    # send the model summary to the summary file
    sink(file = summaryfile,
         append = FALSE,
         type = "output",
         split = FALSE
         )
    
    print(summary(m))
    
    sink()
    
    # send the AIC and BIC to the metrics file
    metrics = sprintf("%s,%f,%f\n", f, AIC(m), BIC(m))
    
    sink(file = metricsfile,
         append = TRUE,
         type = "output",
         split = FALSE)
    
    cat(metrics)
    
    sink()
  }
}
```


### Model selection

```{r}
# read in the AIC/BIC data
fin = sprintf("%s/sqrt_time_model_correct_summaries_only_frozen_re/metrics.csv", outputdir)
log_time_models_da = read_csv(fin)
```
```{r}
head(log_time_models_da[order(log_time_models_da$AIC),], n = 10)
```
```{r}
head(log_time_models_da[order(log_time_models_da$BIC),], n = 10)
```



```{r}
fA = log_time_models_da[order(log_time_models_da$AIC),]$formula[[1]]
fB = log_time_models_da[order(log_time_models_da$BIC),]$formula[[1]]
cat(fB)
```

 
Both AIC and BIC favor the same model

```{r}
m0 = lmer(fA, data=df_correct, REML=FALSE)
summary(m0)
```


Both AIC and BIC agree on the best model =  sqrt(duration) ~ exercise + ai + seq_num + (1 | part_id) + (1 | stim_src_id)



As an aside, we are curious if complexity metrics may explain different response times. 

```{r}
# does adding halstead_volume improve model fit
df_correct$halstead_z <- scale(df_correct$halstead_volume) #we need to rescale it
f_hal =  sqrt(duration) ~ exercise + ai + seq_num + halstead_z + (1 | part_id) + (1 | stim_src_id)
m_hal = lmer(f_hal, data=df_correct, REML=FALSE)

anova(m0, m_hal)


```
Halstead Volume does NOT significantly improve the response-time model.


```{r}
# does adding halstead_volume improve model fit
df_correct$cyclomatic_compl_z <- scale(df_correct$cyclomatic_compl) #we need to rescale it
f_cyc =  sqrt(duration) ~ exercise + ai + seq_num + cyclomatic_compl_z + (1 | part_id) + (1 | stim_src_id)
m_cyc = lmer(f_cyc, data=df_correct, REML=FALSE)

anova(m0, m_cyc)


```

Cyclomatic complexity significantly improved model fit for response time.

cyclomatic_compl  Estimate = -0.18544   t = -2.132 (Participants responded faster on code with higher cyclomatic complexity.)


exerciseTrue        = +1.90881
cyclomatic_compl    = -0.18544

1.98 / 0.18 = 10.3
(Exercising produced a response-time cost comparable in magnitude to the benefit of simplifying a code snippet by roughly 10 units of cyclomatic complexity.)





We now look at our chosen model in depth and determine the p-values of the predictors

```{r}
fC=  sqrt(duration) ~ ai + seq_num +  cyclomatic_compl + (1 | part_id) + (1 | stim_src_id)
fD = sqrt(duration) ~ exercise + seq_num +  cyclomatic_compl + (1 | part_id) + (1 | stim_src_id)
fE = sqrt(duration) ~ exercise + ai +  cyclomatic_compl + (1 | part_id) + (1 | stim_src_id)

mC = lmer(fC, data=df_correct, REML=FALSE)
mD = lmer(fD, data=df_correct, REML=FALSE)
mE = lmer(fE, data=df_correct, REML=FALSE)

```

```{r}

anova(m_cyc ,mC) # Does model favor exercise?
anova(m_cyc ,mD) # Does model favor AI?
anova(m_cyc ,mE) # Does model favor Seq_num (learning effects)
```

m0: sqrt(duration) ~ exercise + ai + seq_num + (1 | part_id) + (1 | stim_src_id)
Adding exercise p = 0.0006 (made people slower)
Adding AI p = 0.2366 (no difference - slightly got faster but not significant)
Adding seq_num p < 0.001 (people got faster as the time went on)



```{r}
summary(m_cyc)
r2(m_cyc)
```



